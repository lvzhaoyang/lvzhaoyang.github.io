<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0053)https://www.cc.gatech.edu/~hays/compvision2017/proj4/ -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Project 4: Scene recognition with bag of words</title>
    <link href="./proj4_files/style.css" rel="stylesheet" type="text/css">
<style type="text/css" media="all">
#primarycontent {
    margin-left: auto;
    width: expression(document.body.clientWidth > 995? "995px": "auto" );
    margin-right: auto;
    text-align: left;
    max-width: 995px
}
</style></head>



<body>
<div id="primarycontent">
<center>
<img src="./proj4_files/header.png"><p style="color: #666;">
An example of a typical bag of words classification pipeline. Figure by <a href="http://www.robots.ox.ac.uk/~vgg/research/encoding_eval/">Chatfield et al.</a></p><p></p></center>

<h1>Project 4: Scene recognition with bag of words<br>
<a href="https://www.cc.gatech.edu/~zlv30/courses/CS4476.html">CS 4476: Computer Vision</a></h1>


<h2>Brief</h2>
<p>
</p><ul>
  <li>Due: 11:59pm on Monday, July 8</li>
  <li>Project materials including writeup template <a href="https://gatech.instructure.com/">Canvas->files->proj4.zip</a>.</li>
  </li><li>Handin: through <a href="https://gatech.instructure.com/">Canvas->Assignment</a> </li>
  <li>Required files: README, code/, code/vocab.pkl, pdf/</li> 
</ul>
<p></p>

<h2>Overview</h2>
<p>
The goal of this project is to introduce you to image recognition. Specifically, we will examine the task of scene recognition starting with very simple methods -- tiny images and nearest neighbor classification -- and then move on to more advanced methods -- bags of quantized local features and linear classifiers learned by support vector machines.
</p><p>
Bag of words models are a popular technique for image classification inspired by models used in natural language processing. The model ignores or downplays word arrangement (spatial information in the image) and classifies based on a histogram of the frequency of visual words. The visual word "vocabulary" is established by clustering a large corpus of local features. See Szeliski chapter 14.4.1 for more details on category recognition with quantized features. In addition, 14.3.2 discusses vocabulary creation and 14.1 covers classification techniques.
</p>
<p>
For this project you will be implementing a basic bag of words model with many opportunities for extra credit. You will classify scenes into one of 15 categories by training and testing on the 15 scene database (introduced in <a href="http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf">Lazebnik et al. 2006</a>, although built on top of previously published datasets). <a href="http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf">Lazebnik et al. 2006</a> is a great paper to read, although we will be implementing the <i>baseline method</i> the paper discusses (equivalent to the zero level pyramid) and not the more sophisticated spatial pyramid (which is extra credit). For an excellent survey of pre-deep-learning feature encoding methods for bag of words models see <a href="http://www.robots.ox.ac.uk/~vgg/research/encoding_eval/">Chatfield et al, 2011</a>.

</p><center><img src="./proj4_files/categories.png">
<p style="color: #666;">Example scenes from of each category in the 15 scene dataset. Figure from <a href="http://www.di.ens.fr/willow/pdfs/cvpr06b.pdf">Lazebnik et al. 2006</a>.</p></center><p></p>

<h2>Setup</h2>
<p><strong>(We will create a new environment named <code>cs4476p4</code> with a lot more dependencies compared to previous projects!)</strong></p>
<ol>
  <li>Install <a href="https://conda.io/miniconda.html">Miniconda</a>. It doesn't matter whether you use 2.7 or 3.6 because we will create our own environment anyways.</li>
  <li>Create a conda environment using the appropriate command. On Windows, open the installed "Conda prompt" to run this command. On MacOS or Linux, you can just use a terminal window to run the command.
    <br>
    <code>conda env create -f environment_&lt;OS&gt;.yml</code>
  </li>
  <li>This should create an environment named <code>cs4476p4</code>. Activate it using the follow Windows command:
    <br>
    <code>activate cs4476p4</code>
    <br>
    or the following MacOS/Linux command:
    <br>
    <code>source activate cs4476p4</code>
  </li>
  <li>Run the notebook using:
    <br>
    <code>jupyter notebook ./code/proj4.ipynb</code>
  </li>
  <li>Generate the submission once you've finished the project using:
    <br>
    <code>python zip_submission.py</code>
  </li>
</ol>

<h2>Details and Starter Code</h2>
<p>
The primary script for this project is <code>student_code.py</code>. The ipython notebook <code>proj4.ipynb</code> runs through the function calls required to implement the project. Your task is to implement the methods within the primary script that will allow you to achieve the desired accuracies on the notebook.
</p>
<p>
You are required to implement 2 different image representations -- tiny images and bags of SIFT features -- and 2 different classification techniques -- nearest neighbor and linear SVM. In the writeup, you are specifically asked to report performance for the following combinations, and it is also highly recommended that you implement them in this order:
</p><ul>
 <li>Tiny images representation and nearest neighbor classifier (accuracy of about 18-25%).</li>
 <li>Bag of SIFT representation and nearest neighbor classifier (accuracy of about 50-60%).</li>
 <li>Bag of SIFT representation and linear SVM classifier (accuracy of about 60-70%).</li>
</ul>

<p>
You will start by implementing the tiny image representation and the nearest neighbor classifier. They are easy to understand, easy to implement, and runs very quickly for our experimental setup (less than 10 seconds).
</p>
<p>
The "tiny image" feature, inspired by the work of the same name by <a href="http://groups.csail.mit.edu/vision/TinyImages/">Torralba, Fergus, and Freeman</a>, is one of the simplest possible image representations. One simply resizes each image to a small, fixed resolution (we recommend 16x16). It works slightly better if the tiny image is made to have zero mean and unit length. This is not a particularly good representation, because it discards all of the high frequency image content and is not especially invariant to spatial or brightness shifts. <a href="http://groups.csail.mit.edu/vision/TinyImages/">Torralba, Fergus, and Freeman</a> propose several alignment methods to alleviate the latter drawback, but we will not worry about alignment for this project. We are using tiny images simply as a  baseline. See <code>get_tiny_images()</code> in the starter code for more details.
</p>
<p>
The nearest neighbor classifier is equally simple to understand. When tasked with classifying a test feature into a particular category, one simply finds the "nearest" training example (L2 distance is a sufficient metric) and assigns the test case the label of that nearest training example. The nearest neighbor classifier has many desirable features -- it requires no training, it can learn arbitrarily complex decision boundaries, and it trivially supports multiclass problems. It is quite vulnerable to training noise, though, which can be alleviated by voting based on the K nearest neighbors (but you are not required to do so). Nearest neighbor classifiers also suffer as the feature dimensionality increases, because the classifier has no mechanism to learn which dimensions are irrelevant for the decision. It also involves exponentially more additions for large number of dimensions and training examples. See <code>nearest_neighbor_classify()</code> for more details.
</p>
<p>
Together, the tiny image representation and nearest neighbor classifier will get about 15% to 25% accuracy on the 15 scene database. For comparison, chance performance is ~7%.
</p>
<p>
After you have implemented a baseline scene recognition pipeline it is time to move on to a more sophisticated image representation -- bags of quantized SIFT features. Before we can represent our training and testing images as bag of feature histograms, we first need to establish a <i>vocabulary</i> of visual words. We will form this vocabulary by sampling many local features from our training set (10's or 100's of thousands) and then clustering them with kmeans. The number of kmeans clusters is the size of our vocabulary and the size of our features. For example, you might start by clustering many SIFT descriptors into k=50 clusters. This partitions the continuous, 128 dimensional SIFT feature space into 50 regions. For any new SIFT feature we observe, we can figure out which region it belongs to as long as we save the centroids of our original clusters. Those centroids are our visual word vocabulary. Because it can be slow to sample and cluster many local features, the starter code saves the cluster centroids and avoids recomputing them on future runs. See <code>build_vocabulary()</code> for more details.
</p>
<p>
Now we are ready to represent our training and testing images as histograms of visual words. For each image we will densely sample many SIFT descriptors. Instead of storing hundreds of SIFT descriptors, we simply count how many SIFT descriptors fall into each cluster in our visual word vocabulary. This is done by finding the nearest neighbor kmeans centroid for every SIFT feature. Thus, if we have a vocabulary of 50 visual words, and we detect 220 SIFT features in an image, our bag of SIFT representation will be a histogram of 50 dimensions where each bin counts how many times a SIFT descriptor was assigned to that cluster and sums to 220. The histogram should be normalized so that image size does not dramatically change the bag of feature magnitude. See <code>get_bags_of_sifts()</code> for more details.
</p>
<p>
You should now measure how well your bag of SIFT representation works when paired with a nearest neighbor classifier. There are <i>many</i> design decisions and free parameters for the bag of SIFT representation (number of clusters, sampling density, sampling scales, SIFT parameters, etc.) so accuracy might vary from 50% to 60%.
</p>
<p>
The last task is to train 1-vs-all linear SVMS to operate in the bag of SIFT feature space. Linear classifiers are one of the simplest possible learning models. The feature space is partitioned by a learned hyperplane and test cases are categorized based on which side of that hyperplane they fall on. Despite this model being far less expressive than the nearest neighbor classifier, it will often perform better. For example, maybe in our bag of SIFT representation 40 of the 50 visual words are uninformative. They simply don't help us make a decision about whether an image is a 'forest' or a 'bedroom'. Perhaps they represent smooth patches, gradients, or step edges which occur in all types of scenes. The prediction from a nearest neighbor classifier will still be heavily influenced by these frequent visual words, whereas a linear classifier can learn that those dimensions of the feature vector are less relevant and thus downweight them when making a decision. There are numerous methods to learn linear classifiers but we will find linear decision boundaries with a support vector machine. You do not have to implement the support vector machine. However, linear classifiers are inherently binary and we have a 15-way classification problem. To decide which of 15 categories a test case belongs to, you will train 15 binary, 1-vs-all SVMs. 1-vs-all means that each classifier will be trained to recognize 'forest' vs 'non-forest', 'kitchen' vs 'non-kitchen', etc. All 15 classifiers will be evaluated on each test case and the classifier which is most confidently positive "wins". E.g. if the 'kitchen' classifier returns a score of -0.2 (where 0 is on the decision boundary), and the 'forest' classifier returns a score of -0.3, and all of the other classifiers are even more negative, the test case would be classified as a kitchen even though none of the classifiers put the test case on the positive side of the decision boundary.  When learning an SVM, you have a free parameter 'lambda' which controls how strongly regularized the model is. Your accuracy will be very sensitive to lambda, so be sure to test many values. See <code>svm_classify()</code> for more details.
</p>
<p>
Now you can evaluate the bag of SIFT representation paired with 1-vs-all linear SVMs. Accuracy should be from 60% to 70% depending on the parameters. You can do better still if you implement extra credit suggestions below.
</p>
<p>
The starter code, starting from <code>student_code.py</code> contains more concrete guidance on the inputs, outputs, and suggested strategies for the five functions you will implement: <code>get_tiny_images()</code>, <code>nearest_neighbor_classify()</code>, <code>build_vocabulary()</code>, <code>get_bags_of_sifts()</code>, and <code>svm_classify()</code>. The utils folder contains code required to visualize the results on the notebook, which you are not required to edit.
</p>

<h2>Experimental Design</h2>
<p>
An important aspect of machine learning is to estimate "good" hyper-parameters. As part of this project, you will perform the following three experiments and analyze the results in your writeup:
<ul>
<li>Use cross-validation to measure performance rather than the fixed test / train split provided by the starter code. Randomly pick 100 training and 100 testing images for each iteration and report average performance and standard deviations.</li>
<li>Add a validation set to your training process to tune learning parameters. This validation set could either be a subset of the training set or some of the otherwise unused test set.</li>
<li>Experiment with many different vocabulary sizes and report performance. E.g. 10, 20, 50, 100, 200, 400, 1000, 10000.</li>
</ul>
</p>

<h2>Evaluation and Visualization</h2>
<p>
The starter code builds a confusion matrix and visualizes your classification decisions by producing a <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix between the ground truth test labels and the predicted test labels</a> within the notebook, each time you run <code>student_code.py</code>.
</p>
<h2>Data</h2>
<p>
The starter codes trains and tests on 100 images from each category (i.e. 1500 training examples total and 1500 test cases total). Unzip the provided data.zip and place the 'data/' folder alongside 'code/'. In a real research paper, one would be expected to test performance on random splits of the data into training and test sets, but the starter code does not do this to ease debugging.
</p>
<h2>Useful Functions</h2>
<p>
The starter code contains more complete discussions of useful functions from Scikit-Learn utilities to VLFeat commands.
</p>

<h2>Write up</h2>
<p>
For this project, and all other projects, you must add a project report compiled using the notebook(proj4.ipynb) under analysis section. In the writeup you will describe your algorithm and any decisions you made to write your algorithm a particular way. Use the experimental design results to aid your analysis. Analysis must be more than just a presentation of results or narration of steps, and can include things like explanations for <strong>why</strong> design choices affect results in certain ways. Discuss any extra credit you did and show what contribution it had on the results (e.g. performance with and without each extra credit component).</p>
<p>
<strong> Make sure to keep ALL your latest results in your ipython notebook and converted pdf file. </strong>


<h2>Extra Credit</h2>
<p>
For all extra credit, be sure to include quantitative analysis showing the impact of the particular method you've implemented. You can only get extra credits with an improved performance compared to the baseline you finished in the previous steps. Each item is "up to" some amount of points because trivial implementations may not be worthy of full extra credit. Most of the extra credit focuses on the final bag of words + SVM pipeline of the project, not the baseline tiny image and nearest neighbor methods.
</p>
<p>
Feature representation extra credit:
</p><ul>
<li>up to 10 pts: Experiment with features at multiple scales. E.g. sampling features from different levels of a Gaussian pyramid.</li>
</ul>
<p>
Feature quantization and bag of words extra credit:
</p><ul>
<li>up to 10 pts: Use "soft assignment" to assign visual words to histogram bins. Each visual word will cast a distance-weighted vote to multiple bins. This is called "kernel codebook encoding" by <a href="http://www.robots.ox.ac.uk/~vgg/research/encoding_eval/">Chatfield et al.</a>.</li>
</ul>
<p>
Classifier extra credit:
</p><ul>
<li>up to 10 pts: Train the SVM with more sophisticated kernels such as Gaussian/RBF, L1, or chi-sqr. Try using the 'kernel' and 'gamma' arguments in scikit-learn's SVC method.
<li>up to 10 pts: Try and improve the nearest neighbor classifier to be competitive or better than the linear SVM using the method of <a href="http://www.wisdom.weizmann.ac.il/~irani/PAPERS/InDefenceOfNN_CVPR08.pdf">Boiman, Schechtman, and Irani, CVPR 2008</a>.
</li></ul>

<p>
We don't recommend exploring deep learning methods as extra credit because that will be the focus of next project (project 5).

</p><p>
Finally, there will be extra credit (5 points) for the student who achieves the highest recognition rate . 

<h2> Handing in </h2>
<p>
This is very important as you will lose points if you do not follow instructions. Every time you do not follow instructions, you will lose 5 points. The folder you hand in must contain the following:
</p>
<ul>
    <li> README.txt - text file containing how to run your bells and whistles and extra credit implementations, along with anything about the project that you want to tell the TAs</li>
    <li> code/ - directory containing all your code for this assignment</li>
    <li> code/vocab.pkl - please make sure the vocabulary file you want us to run your code with is named vocab.pkl</li>
    <li> pdf/ &#60;LastName&#62;_&#60;FirstName&#62;_proj1.pdf - Convert jupyter notebook to pdf, using download as pdf in the jupyter menu.
</ul>
<p>
Hand in your project as a zip file through <a href="https://gatech.instructure.com">Canvas</a>. This zip must be less than 10MB. If images are taking up too much space, you can use things like imagemagick to shrink them. <strong> Make sure the function signature in student_code.py are unchanged.</strong> 
</p>

<h2> Rubric </h2>
<ul>
<li> +10 pts: Build tiny image features for scene recognition. (<code>get_tiny_images()</code>)</li>
<li> +10 pts: Nearest neighbor classifier. (<code>nearest_neighbor_classify()</code>)</li>
   <li> +20 pts: Build a vocabulary from a random set of training features. (<code>build_vocabulary()</code>)</li>
   <li> +20 pts: Build histograms of visual words for training and testing images. (<code>get_bags_of_sifts()</code>)</li>
   <li> +10 pts: Train 1-vs-all SVMs on your bag of words model. (<code>svm_classify()</code>)</li>
   <li> +10 pts: Experimental design credit </li>
   <li> +20 pts: Writeup with design decisions and evaluation</li>
   <li> +30 pts: Extra credit. </li>
   <li> -5*n pts: Lose 5 points for every time you do not follow the instructions for the hand in format </li>
</ul>

<h2> Final Advice </h2>
<p>
Extracting features, clustering to build a universal dictionary, and building histograms from features can be slow. A good implementation can run the entire pipeline in less than 10 minutes, but this may be at the expense of accuracy (e.g. too  small a vocabulary of visual words or too sparse a sampling rate).

</p><h2> Credits </h2>
This project is based on a project from Aaron Bobick's offering of 4495 and has been expanded and edited by Sainandan Ramakrishnan, Vijay Upadhya, Samarth Brahmbhatt and James Hays.Final edited by Zhaoyang Lv and Ankit Arora</p></div>


</body></html>
