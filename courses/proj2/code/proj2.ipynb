{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CS 4476 Project 2: Local Feature Matching](https://www.cc.gatech.edu/~zlv30/courses/CS4476.html)\n",
    "\n",
    "This iPython notebook:  \n",
    "(1) Loads and resizes images  \n",
    "(2) Finds interest points in those images                 (you code this)  \n",
    "(3) Describes each interest point with a local feature    (you code this)  \n",
    "(4) Finds matching features                               (you code this)  \n",
    "(5) Visualizes the matches  \n",
    "(6) Evaluates the matches based on ground truth correspondences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from student_feature_matching import match_features\n",
    "from student_sift import get_features\n",
    "from student_harris import get_interest_points\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('../data/Notre Dame/921919841_a30df938f2_o.jpg')\n",
    "image2 = load_image('../data/Notre Dame/4191453057_c86028ce1f_o.jpg')\n",
    "eval_file = '../data/Notre Dame/921919841_a30df938f2_o_to_4191453057_c86028ce1f_o.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('../data/Mount Rushmore/9021235130_7c2acd9554_o.jpg')\n",
    "# image2 = load_image('../data/Mount Rushmore/9318872612_a255c874fb_o.jpg')\n",
    "# eval_file = '../data/Mount Rushmore/9021235130_7c2acd9554_o_to_9318872612_a255c874fb_o.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('../data/Episcopal Gaudi/4386465943_8cf9776378_o.jpg')\n",
    "# image2 = load_image('../data/Episcopal Gaudi/3743214471_1b5bbfda98_o.jpg')\n",
    "# eval_file = '../data/Episcopal Gaudi/4386465943_8cf9776378_o_to_3743214471_1b5bbfda98_o.pkl'\n",
    "\n",
    "                    \n",
    "scale_factor = 0.5\n",
    "image1 = cv2.resize(image1, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "image2 = cv2.resize(image2, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "image1_bw = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)\n",
    "image2_bw = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "feature_width = 16 # width and height of each local feature, in pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find distinctive points in each image (Szeliski 4.1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1, _, scales1, _ = get_interest_points(image1_bw, feature_width)\n",
    "x2, y2, _, scales2, _ = get_interest_points(image2_bw, feature_width)\n",
    "# x1, y1, x2, y2 = cheat_interest_points(eval_file, scale_factor)\n",
    "# plt.figure(); plt.imshow(image1_bw)\n",
    "\n",
    "# Visualize the interest points\n",
    "c1 = show_interest_points(image1, x1, y1)\n",
    "c2 = show_interest_points(image2, x2, y2)\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.figure(); plt.imshow(c2)\n",
    "print('{:d} corners in image 1, {:d} corners in image 2'.format(len(x1), len(x2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature vectors at each interest point (Szeliski 4.1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_features = get_features(image1_bw, x1, y1, feature_width, scales1)\n",
    "image2_features = get_features(image2_bw, x2, y2, feature_width, scales2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match features (Szeliski 4.1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches, confidences = match_features(image1_features, image2_features, x1, y1, x2, y2)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "You might want to set 'num_pts_to_visualize' and 'num_pts_to_evaluate' to some constant (e.g. 100) once you start detecting hundreds of interest points, otherwise things might get too cluttered. You could also threshold based on confidence.  \n",
    "  \n",
    "There are two visualization functions below. You can comment out one of both of them if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 100\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out the function below if you are not testing on the Notre Dame, Episcopal Gaudi, and Mount Rushmore image pairs--this evaluation function will only work for those which have ground truth available.  \n",
    "  \n",
    "You can use `annotate_correspondences/collect_ground_truth_corr.py` to build the ground truth for other image pairs if you want, but it's very tedious. It would be a great service to the class for future years, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 100\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        x1[matches[:num_pts_to_evaluate, 0]], y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        x2[matches[:num_pts_to_evaluate, 1]], y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(); plt.imshow(c)\n",
    "plt.savefig('../results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Decribe implementation of your interest point detector ? Show the results of your interest point detector on two different images from the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add your description here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for showing the results of the Interest point detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**2) Describe how you implemented Adaptive non-maximum suppression ? Show the effectivness of ANMS on two different images from the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add your description here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for visualizing the impact of ANMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**3) How you are creating SIFT descriptor? Please provide detailed description of your implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add your description here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**4) Provide details on your Feature matching pipline and ratio test? Show the result of your Feature matching pipeline on 'Notre Dam', 'Mount Rushmore' and 'Episcopal' image pairs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add your description here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for showing the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**5) Detailed quantitative analysis showing the impact of two different hyperparameters on overall results? Describe how you tuned them? Illustrate using examples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add your description here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Extra Credit:- Bells and Whistles (Optional)\n",
    "\n",
    "Implementation of bells and whistles can increase your grade by up to 10 points (potentially over 100). The max score for all students is 110.\n",
    "\n",
    "For all extra credit, be sure to include quantitative analysis showing the impact of the particular method you've implemented. Each item is \"up to\" some amount of points because trivial implementations may not be worthy of full extra credit\n",
    "\n",
    "### Interest point detection bells and whistles:\n",
    "\n",
    "+ up to 5 pts: Try detecting keypoints at multiple scales or using a scale selection method to pick the best scale.\n",
    "+ up to 5 pts: Try estimating the orientation of keypoints to make your local features rotation invariant.\n",
    "+ up to 10 pts: Try an entirely different interest point detection strategy like that of MSER. If you implement an additional interest point detector, you can use it alone or you can take the union of keypoints detected by multiple methods.\n",
    "\n",
    "### Local feature description bells and whistles:\n",
    "\n",
    "+ up to 3 pts: The simplest thing to do is to experiment with the numerous SIFT parameters: how big should each feature be? How many local cells should it have? How many orientations should each histogram have? Different normalization schemes can have a significant effect, as well. Don't get lost in parameter tuning, though.\n",
    "+ up to 5 pts: If your keypoint detector can estimate orientation, your local feature descriptor should be built accordingly so that your pipeline is rotation invariant.\n",
    "+ up to 5 pts: Likewise, if you are detecting keypoints at multiple scales, you should build the features at the corresponding scales.\n",
    "\n",
    "### Local feature matching bells and whistles: \n",
    "\n",
    "An issue with the baseline matching algorithm is the computational expense of computing distance between all pairs of features. For a reasonable implementation of the base pipeline, this is likely to be the slowest part of the code. There are numerous schemes to try and approximate or accelerate feature matching: \n",
    "\n",
    "+ up to 10 pts: Use a space partitioning data structure like a kd-tree or some third party approximate nearest neighbor package to accelerate matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit  1 (Optional):-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Provide short discription of your implementation here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Extra Credit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Results of your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Provide quantitative analysis showing the impact of the particular method or hyperparameter.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "### Extra Credit  2 (Optional):-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Provide short discription of your implementation here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Extra Credit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Results of your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Provide quantitative analysis showing the impact of the particular method or hyperparameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
